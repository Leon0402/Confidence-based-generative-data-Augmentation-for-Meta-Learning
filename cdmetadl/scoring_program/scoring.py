# AS A PARTICIPANT, DO NOT MODIFY THIS CODE.
#
# ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED "AS-IS". 
# ISABELLE GUYON, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM
# ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, 
# AND THE WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY'S INTELLECTUAL 
# PROPERTY RIGHTS. IN NO EVENT SHALL ISABELLE GUYON AND/OR OTHER ORGANIZERS BE 
# LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES 
# WHATSOEVER ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF 
# SOFTWARE, DOCUMENTS, MATERIALS, PUBLICATIONS, OR INFORMATION MADE AVAILABLE 
# FOR THE CHALLENGE. 
#
# Main contributors: Dustin Carrión-Ojeda, Ihsan Ullah, Isabelle Guyon, and 
# Sergio Escalera. 
# March-August 2022
#
# This is the "scoring program" written by the organizers. This program also 
# runs on the challenge platform to test your code.
#
# Usage: 
# python scoring.py results_dir output_dir
#
# The results directory results_dir (e.g. sample_result_submission/) must 
# contain the ground truth (.true) the predicted labels (.predict), and the 
# metadata (.pkl) generated by the ingestion program:
# 	task_{task_id}.true
# 	task_{task_id}.predict
#   task_{task_id}_metadata.pkl
#
# The output directory output_dir (e.g. scoring_output/) will store the 
# computed scores


import os
import datetime
import pickle
import jinja2
from sys import exit, version
from absl import app, flags
from cdmetadl.scoring_program.scoring_helpers import *


# =============================== BEGIN OPTIONS ===============================
FLAGS = flags.FLAGS

# Verbose mode 
# True: show various progression messages (recommended value)
# False: no progression messages are shown
flags.DEFINE_boolean("verbose", False, "Verbose mode.")

# Debug mode
# 0: no debug
# 1: compute additional scores (recommended value)
# 2: same as 1 + show the Python version and list the directories 
flags.DEFINE_integer("debug_mode", 2, "Debug mode.")

# Overwrite results flag
# True: the previous output directory is overwritten
# False: the previous output directory is renamed (recommended value)
flags.DEFINE_boolean("overwrite_previous_results", True, 
    "Overwrite results flag.")

# Default location of directories
# If no arguments to scoring.py are provided, these are the directories used. 
# Change the ROOT_DIR to your local directory.
flags.DEFINE_string("output_dir_ingestion","../../../sample_result_submission", 
    "Path to the output directory for the ingestion program.")
flags.DEFINE_string("output_dir_scoring", "../../../scoring_output", 
    "Path to the ourput directory for the scoring program.")

# =============================================================================
# =========================== END USER OPTIONS ================================
# =============================================================================

# Program version
VERSION = 1.1

def scoring(argv) -> None:
    del argv
    
    VERBOSE = FLAGS.verbose
    DEBUG_MODE = FLAGS.debug_mode
    OVERWRITE_PREVIOUS_RESULTS = FLAGS.overwrite_previous_results
    
    # Define the path to the directory
    results_dir = os.path.abspath(FLAGS.output_dir_ingestion)
    output_dir = os.path.abspath(FLAGS.output_dir_scoring)
    
    vprint(f"Using results_dir: {results_dir}", VERBOSE)
    vprint(f"Using output_dir: {output_dir}\n", VERBOSE)
    
    # Show library version and directory structure
    if DEBUG_MODE >= 2: 
        print(f"Python version: {version}")
        print(f"Scoring program version: {VERSION}\n")
        show_dir(".")
    
    vprint(f"{'#'*45}\nScoring Program Starts\n{'-'*45}\n", VERBOSE)
    
    vprint(f"Checking directories", VERBOSE)
    exist_dir(results_dir)
    
    number_of_tasks = (len(os.listdir(results_dir)) - 1) // 3
    if number_of_tasks < 1:
        print(f"There are no results in {results_dir}")
        exit(1)
        
    vprint(f"Computing scores", VERBOSE)
    # Compute the score for each task
    if DEBUG_MODE < 1:
        # Read metric and count the number of tasks
        score_name, scoring_function = get_score()
        vprint(f"Using score: {score_name}", VERBOSE)
    scores = dict()
    scores_per_dataset = dict()
    scores_per_ways = dict()
    scores_per_shots = dict()
    tasks = list()
    for i in range(1, number_of_tasks + 1):
        # Extract task information
        task_name = f"{results_dir}/task_{i}"
        with open(f"{task_name}_metadata.pkl", "rb") as f:
            task_metadata = pickle.load(f)
        
        # Load ground truth and predicted labels
        y_true = read_results_file(f"{task_name}.true", int)
        y_pred = read_results_file(f"{task_name}.predict", float)
    
        # Compute and store the scores
        if DEBUG_MODE < 1:
            task_scores = scoring_function(y_true, y_pred)
            task_scores = {score_name: task_scores}
        else:
            task_scores = compute_all_scores(y_true, y_pred)
        keys = list(task_scores.keys())
        
        task_dataset = task_metadata["dataset"]
        if task_dataset not in scores_per_dataset:
            scores_per_dataset[task_dataset] = {key: [] for key in keys}
        
        task_ways = task_metadata["num_ways"]
        if task_ways not in scores_per_ways:
            scores_per_ways[task_ways] = {key: [] for key in keys}
        
        task_shots = task_metadata["num_shots"]
        if task_shots not in scores_per_shots:
            scores_per_shots[task_shots] = {key: [] for key in keys}
        
        for key in keys:
            if key not in scores:
                scores[key] = list()
            scores[key].append(task_scores[key])
            scores_per_dataset[task_dataset][key].append(task_scores[key])
            scores_per_ways[task_ways][key].append(task_scores[key])
            scores_per_shots[task_shots][key].append(task_scores[key])
            
        tasks.append({
            "dataset": task_dataset,
            "num_ways": task_ways,
            "classes": task_metadata["classes"],
            "num_shots": task_shots,
            "query_size": task_metadata["query_size"],
            "scores": [round(task_scores[key], 2) for key in keys],
            "exec_time": round(task_metadata["exec_time"], 4)
        })
        
    vprint(f"Creating output directory", VERBOSE)
    if not OVERWRITE_PREVIOUS_RESULTS:
        timestamp = datetime.datetime.now().strftime("%y-%m-%d-%H-%M-%S")
        mvdir(output_dir, f"{output_dir}_{timestamp}") 
    mkdir(output_dir) 
    
    vprint(f"Saving scores\n", VERBOSE)
    # Data for html report
    overall_scores = dict()
    scores_grouped_by_dataset = list()
    datasets_heatmaps = dict()
    scores_grouped_by_ways = list()
    ways_heatmaps = dict()
    scores_grouped_by_shots = list()
    shots_heatmaps = dict()
    try:
        plots_dir = os.path.join(output_dir, "plots")
        mkdir(plots_dir)
        with open(os.path.join(output_dir, "scores.txt"), "w") as score_file:
            scores_names = list(scores.keys())
            scores_names_to_save = [name.replace(' ', '_').lower() for name in 
                scores_names]
            
            # Overall scores
            for i, score_name in enumerate(scores_names):
                overall_score, overall_ci = mean_confidence_interval(scores[
                    score_name])
                if i == 0:
                    score_file.write(f"overall_score: {overall_score}\n")
                else:
                    score_file.write(
                        f"overall_{scores_names_to_save[i]}: "
                        + f"{overall_score}\n")
                vprint(f"Overall {score_name}: {overall_score:.2f} ± "+ 
                    f"{overall_ci:.2f}", VERBOSE)
                overall_histogram = create_histogram(scores[score_name], 
                    score_name, f"Overall Frequency Histogram ({score_name})", 
                    f"{plots_dir}/overall_histogram_{scores_names_to_save[i]}") 
                overall_scores[score_name] = {
                    "mean_score": round(overall_score, 2),
                    "ci": round(overall_ci, 2),
                    "histogram": overall_histogram
                }
                
            # Score per dataset     
            vprint("\nScores per dataset", VERBOSE)
            keys = sorted(scores_per_dataset.keys(), key = natural_sort)
            for i, dataset in enumerate(keys):
                dataset_info = {
                    "value": dataset,
                    "mean_score": list(),
                    "ci": list()
                }
                for j, score_name in enumerate(scores_names):
                    curr_info = scores_per_dataset[dataset][score_name]
                    score, conf_int = mean_confidence_interval(curr_info)
                    if j == 0:
                        score_file.write(f"dataset_{i+1}: {score}\n")
                        dataset_info["tasks"] = len(curr_info)
                    else:
                        score_file.write(f"dataset_{i+1}_"+ 
                            f"{scores_names_to_save[j]}: {score}\n")
                    vprint(f"\t{dataset} ({score_name}): {score:.2f} ± "
                        + f"{conf_int:.2f}", VERBOSE)
                    dataset_info["mean_score"].append(
                        round(score, 2))
                    dataset_info["ci"].append(round(conf_int, 2))
                scores_grouped_by_dataset.append(dataset_info)
            for i, score_name in enumerate(scores_names):
                datasets_heatmap = create_heatmap(scores_per_dataset, keys, 
                    keys, score_name, 
                    f"Frequency Heatmap per Dataset ({score_name})", 
                    f"{plots_dir}/heatmap_dataset_{scores_names_to_save[i]}")
                datasets_heatmaps[score_name] = datasets_heatmap
                
            # Score per number of ways      
            vprint("\nScores per number of ways", VERBOSE)
            keys = sorted(scores_per_ways.keys(), key = int)
            for n_ways in keys:
                ways_info = {
                    "value": n_ways,
                    "mean_score": list(),
                    "ci": list()
                }
                for j, score_name in enumerate(scores_names):
                    curr_info = scores_per_ways[n_ways][score_name]
                    score, conf_int = mean_confidence_interval(curr_info)
                    if j == 0:
                        ways_info["tasks"] = len(curr_info)
                    score_file.write(f"{n_ways}_ways_"
                        + f"{scores_names_to_save[j]}: {score}\n")
                    vprint(f"\t{n_ways}-ways ({score_name}): {score:.2f} ± "
                        + f"{conf_int:.2f}", VERBOSE)
                    ways_info["mean_score"].append(round(
                        score, 2))
                    ways_info["ci"].append(round(conf_int, 2))
                scores_grouped_by_ways.append(ways_info)
            for i, score_name in enumerate(scores_names):    
                ways_heatmap = create_heatmap(scores_per_ways, keys, 
                    [f"{key}-ways" for key in keys], score_name, 
                    f"Frequency Heatmap per Number of Ways ({score_name})", 
                    f"{plots_dir}/heatmap_way_{scores_names_to_save[i]}")
                ways_heatmaps[score_name] = ways_heatmap
                
            # Score per number of shots        
            vprint("\nScores per number of shots", VERBOSE)
            keys = sorted(scores_per_shots.keys(), key = int)
            for k_shots in keys:
                shots_info = {
                    "value": k_shots,
                    "mean_score": list(),
                    "ci": list()
                }
                for j, score_name in enumerate(scores_names):
                    curr_info = scores_per_shots[k_shots][score_name]
                    score, conf_int = mean_confidence_interval(curr_info)
                    if j == 0:
                        shots_info["tasks"] = len(curr_info)
                    score_file.write(f"{k_shots}_shot_"
                        + f"{scores_names_to_save[j]}: {score}\n")
                    vprint(f"\t{k_shots}-shots ({score_name}): {score:.2f} ± "
                        + f"{conf_int:.2f}", VERBOSE)
                    shots_info["mean_score"].append(round(
                        score, 2))
                    shots_info["ci"].append(round(conf_int, 2))
                scores_grouped_by_shots.append(shots_info)
            for i, score_name in enumerate(scores_names):    
                shots_heatmap = create_heatmap(scores_per_shots, keys, 
                    [f"{key}-shot" for key in keys], score_name, 
                    f"Frequency Heatmap per Number of Shots ({score_name})", 
                    f"{plots_dir}/heatmap_shot_{scores_names_to_save[i]}")
                shots_heatmaps[score_name] = shots_heatmap
                
            # Global metadata information
            metadata_file = os.path.join(results_dir, "metadata_ingestion")
            if os.path.exists(metadata_file):
                metadata = load_yaml(metadata_file)
            total_tasks = metadata["Tasks"]
            total_datasets = metadata["Number of datasets"]
            score_file.write(f"duration: {metadata['Total execution time']}")
    except Exception as e:
        raise Exception(f"Error while saving scores. Detailed error:{repr(e)}")
    
    # Create HTML report
    try:
        subs = jinja2.Environment(
            loader=jinja2.FileSystemLoader(os.path.dirname(__file__))
        ).get_template("template.html").render(
            title="Results Report",
            scores_names=scores_names,
            overall_scores=overall_scores,
            total_tasks=total_tasks,
            total_datasets=total_datasets,
            scores_grouped_by_dataset=scores_grouped_by_dataset,
            datasets_heatmaps=datasets_heatmaps,
            scores_grouped_by_ways=scores_grouped_by_ways,
            ways_heatmaps=ways_heatmaps,
            scores_grouped_by_shots=scores_grouped_by_shots,
            shots_heatmaps=shots_heatmaps,
            tasks=tasks
        )

        html_file = os.path.join(output_dir, "detailed_results.html")
        with open(html_file, 'w', encoding="utf-8") as f: 
            f.write(subs)    
    except Exception as e:
        print(f"Error while creating HTML report. Detailed error: {repr(e)}")
    
    vprint(f"\n{'-'*45}\nScoring Program Finished Successfully\n{'#'*45}\n", 
        VERBOSE)
    

if __name__ == "__main__":
    app.run(scoring)