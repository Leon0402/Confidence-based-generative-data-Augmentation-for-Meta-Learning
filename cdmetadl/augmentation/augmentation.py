import torch

import cdmetadl.dataset
import cdmetadl.helpers.general_helpers

class Augmentation():
    
    def __init__(self, support_set: tuple[torch.Tensor, torch.Tensor, torch.Tensor], conf_scores: list[float], threshold: float, scale: int):
        self.support_set = support_set
        self.conf_support_set = conf_support_set
        self.conf_scores = conf_scores
        self.threshold = threshold
        self.scale = scale
       


class PseudoAug(Augmentation):
# conf_scores: get confidence scores calculated from reference set (which is a "horizontal" split of image_dataset of test set) averaged over all tasks as well as particular task 
# generated by testgenerator
# aug_factor: if confidence is below a threshold, how many images to we want to add relative to the ones we have e.g factor of 2 means double the images for classes, could be variable and based on how low confidence actually is. total number of samples in task rounded if uneven
# task: test task from testgenerator of N*K*image_dim dimension
# confidence_threshold: float indicating below which confidence value to do augmentation
# returns modified augmentated_task(which inherits from task) with added images sampled from image_dataset the task is from (maybe pass this information)
# could contain dubplicates in one test augmented_task due to sampling randomness

    def __init__(self, support_set: tuple[torch.Tensor, torch.Tensor, torch.Tensor], conf_support_set: tuple[torch.Tensor, torch.Tensor, torch.Tensor], conf_scores: list[float], threshold: float, scale: int, num_shots: int, num_ways: int):
        super().__init__(support_set, conf_scores, threshold, scale)
        self.conf_support_set = conf_support_set

        rearranged_conf_support = [conf_support_set[0].reshape(num_ways, num_shots, 3, 128, 128), conf_support_set[1].reshape(num_ways, num_shots), conf_support_set[2].reshape(num_ways, num_shots)]

    
    def getDatasetAugmented(): 
        shots = list()
        samples_idxs = list()

        for idx, score in enumerate(self.conf_scores): 
            # calculat amounts of samples to samples for this class
            if score < self.threshold:        
                # randomly sample: scale * 1/score * nr_shots from this class/way from conf_support_set
                nr_samples = 1/score * nr_shots * scale
                shots.append(nr_samples + num_shot)
                # add to support_set and return augmented dataset  
                sample_idxs.append(np.random.choice(0, nr_samples))
                
        support_images = torch.stack([
                rearranged_conf_support[idx][i] for i in way_idxs for j, way_idxs in enumerate(sample_idxs)
        ])

        # labe here would be index of class/way
        augmented_support_set = (support_set[0].cat(support_images), torch.tensor(np.arange(n_way).repeat(shots)), torch.tensor(selected_classes.repeat(shots)))
        return augmented_support_set, shots




class StandardAug(Augmentation):

    def __init__(self, task: Task, confidence_threshold: float, aug_factor: int, conf_scores: list[float]):
        super().__init__(task, confidence_threshold, aug_factor, conf_scores)



class GenerativeAug(Augmentation):
    def __init__(self, task: Task, confidence_threshold: float, aug_factor: int, conf_scores: list[float]):
        super().__init__(task, confidence_threshold, aug_factor, conf_scores)
       

    