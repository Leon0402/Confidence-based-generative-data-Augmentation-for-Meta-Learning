{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width:2px;border-color:#84C7F7\">\n",
    "<center><h1> Cross-Domain MetaDL Competition </h1></center>\n",
    "<center><h2>  Any-way Any-shot Learning </h2></center>\n",
    "<hr style=\"border-width:2px;border-color:#84C7F7\">\n",
    "\n",
    "Make sure you have installed all the dependencies (`requirements.txt` and `cdmetadl`) in your kernel environment. If you ran the `quick_start.sh` script, make sure you activated the **cdml** conda environment before launching the jupyter notebook. Here is the link of the [Codalab competition](https://codalab.lisn.upsaclay.fr/competitions/3627?secret_key=2d7c4b66-afa5-4c15-92cb-552f8187245c) where you can submit your code and check the leaderboard.\n",
    "\n",
    "\n",
    "**Outline**: \n",
    "- [**I - Data exploration**](#0): Definition of the any-way any-shot learning setup and exploration of how the data is formatted.\n",
    "- [**II - Submission details**](#1): Explanation of how a submission should be organized.\n",
    "- [**III - Test and submission**](#2): Example of how to locally test a potential submission and also how to zip your scripts to submit your code on CodaLab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "# I - Data exploration\n",
    "\n",
    "The goal of this section is to familiarize participants with the data format used in the challenge.\n",
    "\n",
    "Cross-Domain Meta-Learning aims to produce a **Learner** that is able to quickly adapt to new tasks from multiple domains using only a few examples. In the **standard Machine Learning** setting, we usually split the data in train/valid/test sets, these datasets then contain examples assumed to be generated from **the same distribution**. In **within domain few-shot learning**, we have the same idea but with one additional level of abstraction, *i.e.*, instead of regular data splits we have meta-splits (meta-train/meta-valid/meta-test). In this few-shot learning setting, the meta-splits are assumed to have classes generated from **the same task distribution** similarly to standard Machine Learning. However, in the **cross-domain few-shot learning**, the meta-splits are generated from **different task distributions**. To simulate the cross-domain scenario we provide [10 datasets](https://codalab.lisn.upsaclay.fr/competitions/3627?secret_key=2d7c4b66-afa5-4c15-92cb-552f8187245c#participate) during the public phase of this competition. All these datasets belong to different domains and can be used to locally test your potential submissions.\n",
    "\n",
    "During the competition we generate on the fly the meta-training, meta-validation and meta-testing splits. \n",
    "* **Meta-training**: It is used to meta-train your **MetaLearner**, *i.e.*, try to learn the best approach to tackle different tasks from several domains.\n",
    "* **Meta-validation (optional)**: It can be used to evaluate your **MetaLearner** without worrying about any data leakage.\n",
    "* **Meta-testing**: We will use it to evaluate your Learner's ability to quickly adapt to new unseen any-way any-shot tasks.\n",
    "\n",
    "Let's formalize some of the ideas exposed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "In this competition there are 2 different data formats that can be selected during meta-training. You can either generate data in the form of **tasks** or **batches**. Let's first describe these 2 data formats: \n",
    "\n",
    "A **task**, which represents a ***N*-way *k*-shot task**, is described as follows: \n",
    "$$ \\mathcal{T_j} = \\{ \\mathcal{D}_{\\mathcal{T_j}}^{train}, \\mathcal{D}_{\\mathcal{T_j}}^{test}\\}$$\n",
    "where $\\mathcal{D}_{\\mathcal{T_j}}^{train}$ corresponds to the *support set* that contains the training examples for $\\mathcal{T_j}$, and $\\mathcal{D}_{\\mathcal{T_j}}^{test}$ is the *query set* that contains the test examples for $\\mathcal{T_j}$. Since this competition is focused on the cross-domain few-shot learning setting, the data contained in one task belongs extrictly to one dataset, but different tasks may come from different datasets because the meta-training split is composed by multiple datasets, *i.e.*, $\\mathcal{M}_{\\mathcal{D}}^{train} = \\{\\mathcal{D}_1, \\dots, \\mathcal{D}_n\\}$. The number of datasets ($n$) in the meta-training split $\\mathcal{M}_{\\mathcal{D}}^{train}$ depends on the number of datasets you want to use for the meta-validation split. During the **public phase** you will have **5 datasets** that you can use for meta-training and meta-validation (*e.g.*  $\\mathcal{M}_{\\mathcal{D}}^{train} = \\{\\mathcal{D}_1, \\mathcal{D}_2, \\mathcal{D}_3\\}$ and $\\mathcal{M}_{\\mathcal{D}}^{valid} = \\{\\mathcal{D}_4, \\mathcal{D}_5\\}$) because the remaining 5 datasets are used by the meta-testing split ($\\mathcal{M}_{\\mathcal{D}}^{test} = \\{\\mathcal{D}_6, \\dots, \\mathcal{D}_{10}\\}$). However, during the **feedback and final phases** you will have **10 datasets** that you can use for meta-training and meta-validation.\n",
    "\n",
    "\n",
    "A **batch** is a collection of sampled examples from a dataset **without enforcing a configuration**. Thus, there would be no aforementionned $ \\mathcal{D}_{\\mathcal{T_j}}^{test}$ unlike the task setting. Note that in the current competition, the data contained in one batch may be sampled from multiple of datasets because we concatenate all meta-training classes and corresponding data of the meta-training split ($\\mathcal{M}_{\\mathcal{D}}^{train} = \\{\\mathcal{D}_1, \\dots, \\mathcal{D}_n\\}$) to create a single dataset from which the batches of data are sampled, *i.e.*, $\\mathcal{D}^{train} = concat(\\mathcal{D}_1, \\dots, \\mathcal{D}_n)$.\n",
    "\n",
    "The figure below illustrates the difference between the **task** setting and the **batch** setting.\n",
    "\n",
    "<center>\n",
    "<img src=\"train_settings.png\" alt=\"Train settings\" width=\"700\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The any-way any-shot learning problem\n",
    "\n",
    "The few-shot learning problems are often referred as *N*-way *k*-shots problems. This name refers to the configuration of the tasks at **meta-test time**. The number of **ways** *N* denotes the number of classes in a task that represents an image classification problem. The number of **shots** *k* denotes the number of examples per class in the **support set**. In our case, we focus on the **any-way any-shot** setting. In other words, the tasks at meta-test time represent image classification problems with a number of classes varying from 2 to 20, and the **support set** contains 1 to 20 labeled examples per class, *i.e.*, $N \\in [2, 20]$, and $k \\in [1, 20]$. Thus, at meta-test time your submission may be tested in the following way:\n",
    "- **Test task 1:** 5-ways 1-shot task from Dataset 9.\n",
    "- **Test task 2:** 3-ways 15-shots task from Dataset 3.\n",
    "- **Test task 3:** 12-ways 4-shots task from Dataset 9.\n",
    "- **Test task 4:** 2-ways 8-shots task from Dataset 8.\n",
    "- $\\vdots$\n",
    "\n",
    "Let's summarize the different parts of the meta-learning procedure.\n",
    "\n",
    "* At **meta-train** time: This is the part you control the most. You can choose to generate data from the meta-train split in the form of **tasks** or **batches**. If you choose the task setting, you can specify the *N*-way *k*-shot configuration for the generated tasks. *N* must to be fixed, but you can define an *N*-way *any*-shot configuration by specifying the boundaries for *k*. Addtionally, you can specify how many images per class you want to have in the *query set* of the generated tasks. On the other hand, if you chose the batch setting, you just have to specify the batch size.\n",
    "* At **meta-validation** time: You still have control at this stage, but the generated data is always in the form of tasks. Thus, as before, you can specify the *N*-way *k*-shot configuration you want to use including the images per class for the query set. However, for this stage you can define an *any*-way *any*-shot configuration by specifying the boundaries for *N* and *k*.\n",
    "* At **meta-test** time: You have no control. We always evaluate your submission using any-way any-shot tasks with $N \\in [2, 20]$, and $k \\in [1, 20]$ and 20 examples per class for the query set. \n",
    "\n",
    "As we mentioned previously, in this competition, the tasks/batches are generated **on the fly** from our datasets, but we use the same random seed so each time you make a submission, it will be evaluated with the same data. Also, it is  worth mentioning that the tasks and batches come from **generators**, meaning that there are virtually infinite. \n",
    " \n",
    "**Note**: Make sure you have downloaded the public datasets under `public_data/` directory in the root directory of this project, i.e. `../cd-metadl`. If you used the `quick_start.sh` script, it automatically downloads the public data. \n",
    "\n",
    "Let's see how it looks like in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers to visualize the tasks and batches\n",
    "# DO NOT MODIFY THIS CODE\n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_task(support_images: Tensor, \n",
    "              support_labels: Tensor, \n",
    "              query_images: Tensor,\n",
    "              query_labels: Tensor, \n",
    "              size_multiplier: float = 2, \n",
    "              max_imgs_per_col: int = 10,\n",
    "              max_imgs_per_row: int = 10) -> None:\n",
    "    \"\"\" Plots the content of a task. Tasks are composed of a support set \n",
    "    (training set) and a query set (test set). \n",
    "    \n",
    "    Args:\n",
    "        support_images (Tensor): Images in the support set, they have a \n",
    "            shape of [support_set_size x channels x height x width].\n",
    "        support_labels (Tensor): Labels in the support set, they have a \n",
    "            shape of [support_set_size]. \n",
    "        query_images (Tensor): Images in the query set, they have a \n",
    "            shape of [query_set_size x channels x height x width].\n",
    "        query_labels (Tensor): Labels in the query set, they have a \n",
    "            shape of [query_set_size]. \n",
    "        size_multiplier (float, optional): Dilate or shrink the size of \n",
    "            displayed images. Defaults to 2.\n",
    "        max_imgs_per_col (int, optional): Number of images in a column. \n",
    "            Defaults to 10.\n",
    "        max_imgs_per_row (int, optional): Number of images in a row. Defaults \n",
    "            to 10.\n",
    "    \"\"\"\n",
    "    support_images = np.moveaxis(support_images.numpy(), 1, -1)\n",
    "    support_labels = support_labels.numpy()\n",
    "    query_images = np.moveaxis(query_images.numpy(), 1, -1)\n",
    "    query_labels = query_labels.numpy()\n",
    "\n",
    "    for name, images, class_ids in zip((\"Support\", \"Query\"),\n",
    "                                     (support_images, query_images),\n",
    "                                     (support_labels, query_labels)):\n",
    "        n_samples_per_class = Counter(class_ids)\n",
    "        n_samples_per_class = {k: min(v, max_imgs_per_col) \n",
    "            for k, v in n_samples_per_class.items()}\n",
    "        id_plot_index_map = {k: i for i, k\n",
    "            in enumerate(n_samples_per_class.keys())}\n",
    "        num_classes = min(max_imgs_per_row, len(n_samples_per_class.keys()))\n",
    "        max_n_sample = max(n_samples_per_class.values())\n",
    "        figwidth = max_n_sample\n",
    "        figheight = num_classes\n",
    "        figsize = (figheight * size_multiplier, figwidth * size_multiplier)\n",
    "        fig, axarr = plt.subplots(figwidth, figheight, figsize=figsize)\n",
    "        fig.suptitle(f\"{name} Set\", size='15')\n",
    "        fig.tight_layout(pad=3, w_pad=0.1, h_pad=0.1)\n",
    "        reverse_id_map = {v: k for k, v in id_plot_index_map.items()}\n",
    "        for i, ax in enumerate(axarr.flat):\n",
    "            ax.patch.set_alpha(0)\n",
    "            # Print the class ids, this is needed since, we want to set the x \n",
    "            # axis even there is no picture.\n",
    "            ax.set(xlabel=reverse_id_map[i % figheight], xticks=[], yticks=[])\n",
    "            ax.label_outer()\n",
    "        for image, class_id in zip(images, class_ids):\n",
    "            # First decrement by one to find last spot for the class id.\n",
    "            n_samples_per_class[class_id] -= 1\n",
    "            # If class column is filled or not represented: pass.\n",
    "            if (n_samples_per_class[class_id] < 0 or\n",
    "                id_plot_index_map[class_id] >= max_imgs_per_row):\n",
    "                continue\n",
    "            # If width or height is 1, then axarr is a vector.\n",
    "            if axarr.ndim == 1:\n",
    "                ax = axarr[n_samples_per_class[class_id] \n",
    "                    if figheight == 1 else id_plot_index_map[class_id]]\n",
    "            else:\n",
    "                ax = axarr[n_samples_per_class[class_id], \n",
    "                    id_plot_index_map[class_id]]\n",
    "            ax.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "def plot_batch(images: Tensor, \n",
    "               labels: Tensor, \n",
    "               size_multiplier: int = 1) -> None:\n",
    "    \"\"\" Plot the images in a batch.\n",
    "\n",
    "    Args:\n",
    "        images (Tensor): Images inside the batch, they have a shape of \n",
    "            [batch_size x channels x height x width].\n",
    "        labels (Tensor): Labels inside the batch, they have a shape of\n",
    "            [batch_size].\n",
    "        size_multiplier (int, optional): Dilate or shrink the size of \n",
    "            displayed images. Defaults to 1.\n",
    "    \"\"\"\n",
    "    images = np.moveaxis(images.numpy(), 1, -1)\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    num_examples = len(labels)\n",
    "    figwidth = np.ceil(np.sqrt(num_examples)).astype('int32')\n",
    "    figheight = num_examples // figwidth\n",
    "    figsize = (figwidth * size_multiplier, (figheight + 2.5) * size_multiplier)\n",
    "    _, axarr = plt.subplots(figwidth, figheight, dpi=150, figsize=figsize)\n",
    "\n",
    "    for i, ax in enumerate(axarr.transpose().ravel()):\n",
    "        ax.imshow(images[i])\n",
    "        ax.set(xlabel=str(labels[i]), xticks=[], yticks=[])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will show an example of the **task** setting and after that an example of the batch setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdmetadl.helpers.general_helpers import prepare_datasets_information\n",
    "from cdmetadl.ingestion.image_dataset import create_datasets\n",
    "from cdmetadl.ingestion.data_generator import CompetitionDataLoader\n",
    "\n",
    "public_data_dir = \"../public_data\" # Path to Public data\n",
    "seed = 93 # Random seed to be used\n",
    "\n",
    "# First we need to read the information from the datasets contained in the \n",
    "# pulic data directory \n",
    "datasets_info, _, _ = prepare_datasets_information(input_dir=public_data_dir, \n",
    "                                                   validation_datasets=0, \n",
    "                                                   seed=seed)\n",
    "\n",
    "# Using the retrieved information we can generate Pytorch Datasets\n",
    "datasets = create_datasets(datasets_info)\n",
    "\n",
    "# Now you can define your task configuration \n",
    "generator_config = {\n",
    "    \"N\": 5, # Number of classes for the generated tasks. If you want to \n",
    "            # generate any-way tasks, then put \"N\": None and define the \n",
    "            # boundaries (\"min_N\" and \"max_N\")\n",
    "    \"min_N\": None,\n",
    "    \"max_N\": None,\n",
    "    \"k\": 2, # Number of examples per class for the generated tasks. If you want\n",
    "            # to generate any-shot tasks, then put \"k\": None and define the \n",
    "            # boundaries (\"min_k\" and \"max_k\")\n",
    "    \"min_k\": None,\n",
    "    \"max_k\": None,\n",
    "    \"query_images_per_class\": 4\n",
    "}\n",
    "\n",
    "# Using the generated Pytorch datasets and the defined configuration we can \n",
    "# initialize the data loader\n",
    "loader = CompetitionDataLoader(datasets=datasets, \n",
    "                               episodes_config=generator_config, \n",
    "                               seed=seed)\n",
    "\n",
    "# The initialize data loader has the data generator\n",
    "generator = loader.generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, we follow all the required procedure to created a `CompetitionDataLoader` object to extract the data generator. Now you can visualize the configuration you have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tasks_to_visualize = 2\n",
    "\n",
    "for i, task in enumerate(generator(number_of_tasks_to_visualize)):\n",
    "    print(f\"Task {i+1} from Dataset {task.dataset}\")\n",
    "    print(f\"# Ways: {task.num_ways}\")\n",
    "    print(f\"# Shots: {task.num_shots}\")\n",
    "    plot_task(support_images=task.support_set[0], \n",
    "              support_labels=task.support_set[1],\n",
    "              query_images=task.query_set[0], \n",
    "              query_labels=task.query_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figures above, you can observe the composition of a task: A **support set** (train) and a **query set** (test). In the next cell, we present some useful caracteristics of a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The task object is organized the following way:\\n\" \n",
    "      + \"Task (t):\\n\"\n",
    "      + \"\\t- t.num_ways: int\\n\"\n",
    "      + \"\\t- t.num_shots: int\\n\"\n",
    "      + \"\\t- t.support_set: Tuple[torch.Tensor, torch.Tensor]\\n\"\n",
    "      + \"\\t- t.query_set: Tuple[torch.Tensor, torch.Tensor]\\n\"\n",
    "      + \"\\t- t.original_class_idx: np.ndarray\\n\"\n",
    "      + \"\\t- t.dataset: str\")\n",
    "print(f\"\\n{'#'*70}\\n\")\n",
    "print(\"The support set images are of the following shape: \"\n",
    "    + f\"{task.support_set[0].shape}\")\n",
    "print(f\"The support set labels are: {task.support_set[1].unique()} and \"\n",
    "    + f\"their shape: {task.support_set[1].shape}\")\n",
    "print(f\"\\n{'#'*70}\\n\")\n",
    "print(\"The query set images are of the following shape: \"\n",
    "    + f\"{task.query_set[0].shape}\")\n",
    "print(f\"The query set labels shape is: {task.query_set[1].shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the **batch** setting. \n",
    "\n",
    "Let's assume we would like to receive data from the meta-train split in batches of 20 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Generator\n",
    "from torch.utils.data import DataLoader\n",
    "from cdmetadl.helpers.general_helpers import prepare_datasets_information\n",
    "from cdmetadl.helpers.ingestion_helpers import cycle\n",
    "from cdmetadl.ingestion.image_dataset import ImageDataset\n",
    "from cdmetadl.ingestion.data_generator import CompetitionDataLoader\n",
    "\n",
    "public_data_dir = \"../public_data\" # Path to Public data\n",
    "seed = 93 # Random seed to be used\n",
    "batch_size = 20 # You can define the batch size that you want to use\n",
    "\n",
    "# First we need to read the information from the datasets contained in the \n",
    "# pulic data directory \n",
    "datasets_info, _, _ = prepare_datasets_information(input_dir=public_data_dir, \n",
    "                                                   validation_datasets=0, \n",
    "                                                   seed=seed)\n",
    "\n",
    "# We initialize a random generator to ensure that the same splits are used in\n",
    "# all submissions\n",
    "g = Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "# As explained before, we concatenate all the datasets of the split\n",
    "concatenated_dataset = ImageDataset(datasets_info)\n",
    "num_classes = len(concatenated_dataset.idx_per_label)\n",
    "\n",
    "# Using the concatenated dataset and the defined batch size we can initialize \n",
    "# the data loader\n",
    "loader = DataLoader(dataset=concatenated_dataset, \n",
    "                    batch_size=batch_size, \n",
    "                    shuffle=True, \n",
    "                    generator=g)\n",
    "\n",
    "# Lastly, with the initialized data loader we can define our data generator\n",
    "generator = lambda batches: iter(cycle(batches, loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, we follow all the required procedure to created a `DataLoader` object to define our batch data generator. Now you can visualize the batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batches_to_visualize = 2\n",
    "\n",
    "for i, batch in enumerate(generator(number_of_batches_to_visualize)):\n",
    "    print(f\"Batch {i+1}\")\n",
    "    images, labels = batch\n",
    "    plot_batch(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figures above, you can observe the composition of a batch. In the next cell, we present some useful caracteristics of a this data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The batch object is organized the following way:\\n\" \n",
    "      + \"Batch (b):\\n\"\n",
    "      + \"\\t- b[0]: torch.Tensor (images)\\n\"\n",
    "      + \"\\t- b[1]: torch.Tensor (labels)\")\n",
    "print(f\"\\n{'#'*70}\\n\")\n",
    "print(f\"The images are of the following shape: {batch[0].shape}\")\n",
    "print(f\"The labels are of the following shape: {batch[1].shape}\")\n",
    "print(f\"There is a total of {num_classes} classes in the concatenated dataset.\"\n",
    "      +\" Thus the batches can contain images from all these classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the competition you don't need to create your loaders as shown in the previous examples. You will receive the meta-train and meta-valid generators already initialized and ready to use. The way you receive the data generators will be described in the next section. The default setting for the meta-train data is 5-ways any-shot tasks with $k \\in [1, 20]$ and 20 images per class for the query set. Similarly, the default setting for the meta-valid data is any-way any-shot tasks with $N \\in [2, 20]$, $k \\in [1, 20]$ and 20 images per class for the query set. However, if you think you could achieve better performance with your own meta-training and meta-validation setting, you can specify it. In order to specify your own setting, you need to write down your settings in a single json file named `config.json` and put it in your submission folder before zipping it. We will go over the structure of submission folder in the next sections. Here is an example of a config file:\n",
    "\n",
    "**Content of a `config.json` file**:\n",
    "```bash\n",
    "{\n",
    "    \"train_data_format\": \"task\",\n",
    "    \"batch_size\": null,\n",
    "    \"train_config\": {\n",
    "        \"N\": 10,\n",
    "        \"k\": null,\n",
    "        \"min_k\": 5,\n",
    "        \"max_k\": 10,\n",
    "        \"query_images_per_class\": 5\n",
    "    },\n",
    "    \"validation_datasets\": 2,\n",
    "    \"valid_config\": {\n",
    "        \"N\": null,\n",
    "        \"min_N\": 2,\n",
    "        \"max_N\": 5,\n",
    "        \"k\": 20,\n",
    "        \"min_k\": null,\n",
    "        \"max_k\": null,\n",
    "        \"query_images_per_class\": 10\n",
    "    }\n",
    "}\n",
    "```\n",
    "In the above example configuration you have configured:\n",
    "\n",
    "* **Meta-training:** 10-ways any-shot tasks with with $k \\in [5, 10]$ and 5 images per class for the query set.\n",
    "\n",
    "* **Meta-validation:** any-way 20-shots tasks with with $N \\in [2, 5]$ and 10 images per class for the query set.\n",
    "\n",
    "Additionally, 2 of the available datasets will be used for the meta-validation split and the remaining datasets will be used for the meta-training split. For clarity the available configurations are:\n",
    "\n",
    "- `train_data_format`: Format for the training data, it can be \"task\" or \"batch\".\n",
    "- `batch_size`: Batch size for the generated batches. Only used if `train_data_format` is \"batch\". It cannot be less than 1.\n",
    "- `train_config`: Configuration for the training data. Only used if `train_data_format` is \"task\".\n",
    "  - `N`: Fixed number of ways for the generated tasks at meta-train time. It cannot be less than 2.\n",
    "  - `k`: Fixed number of shots for the generated tasks at meta-train time. If you would like to use any-shot configuration, then you have to define this parameter as `null`. It cannot be less than 1.\n",
    "  - `min_k`: Lower bound for the number of shots for the generated tasks at meta-train time. Only used if `k` is `null`. It cannot be less than 1 and must be less than or equal to `max_k`.\n",
    "  - `max_k`: Upper bound for the number of shots for the generated tasks at meta-train time. Only used if `k` is `null`. It must be greater or equal to `min_k`.\n",
    "  - `query_images_per_class`: Number of examples per class to include in the query set at meta-train time. It cannot be greater than 20.\n",
    "- `validation_datasets`: Number of datasets to be used for the meta-valid split. It can be `null`, but in that case, the `meta_valid_generator` that you will receive will be `None`.\n",
    "- `valid_config`: Configuration for the training data. Only used if `train_data_format` is \"task\".\n",
    "  - `N`: Fixed number of ways for the generated tasks at meta-validation time. It cannot be less than 2.\n",
    "  - `min_N`: Lower bound for the number of ways for the generated tasks at meta-validation time. Only used if `N` is `null`. It cannot be less than 2 and must be less than or equal to `max_N`.\n",
    "  - `max_N`: Upper bound for the number of ways for the generated tasks at meta-validation time. Only used if `N` is `null`. It must be greater or equal to `min_N`.\n",
    "  - `k`: Fixed number of shots for the generated tasks at meta-validation time. If you would like to use any-shot configuration, then you have to define this parameter as `null`. It cannot be less than 1.\n",
    "  - `min_k`: Lower bound for the number of shots for the generated tasks at meta-validation time. Only used if `k` is `null`. It cannot be less than 1 and must be less than or equal to `max_k`.\n",
    "  - `max_k`: Upper bound for the number of shots for the generated tasks at meta-validation time. Only used if `k` is `null`. It must be greater or equal to `min_k`.\n",
    "  - `query_images_per_class`: Number of examples per class to include in the query set at meta-validation time. It cannot be greater than 20.\n",
    "\n",
    "Note that for the meta-training config, you cannot specify `min_N` and `max_N` because as previously explained, during this stage, only fixed number of ways can be used. Additionally, if the configurations that you specify are greater than the maximum number of classes in case of `N` or the maximum number of examples per class in case of `k`, these values will be automatically adjusted to the data available. \n",
    "\n",
    "<span style=\"color:red\">**IMPORTANT:**</span> All the datasets have 40 examples per class, but the number of classes varies for each dataset, the minimum is 19 and the maximum is 706.\n",
    "\n",
    "---\n",
    "\n",
    "**Section summary** :\n",
    "\n",
    "* You can choose to generate data from the meta-train split in the form of tasks or batches. Default configurations are tasks but you can change it via a **config.json** file that you put in your folder submission.\n",
    "* You can choose the configuration for the tasks coming from the meta-validation split (in case you decide to use a meta-validation split). However, we do not allow you to generate batches from this split, only tasks can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# II - Submission details\n",
    "In this section, we will review the structure of a valid submission. We will see that the data we receive for the learning algorithm follows the aforementioned structure.\n",
    "\n",
    "The participants will have to submit a zip file containing one or several files. The crucial file to add is `model.py`. It contains the meta-learning algorithm logic. This file **must** follow the specific API that we defined for the competition described in the following figure: \n",
    "\n",
    "<center>\n",
    "<img src=\"API.png\" alt=\"Challenge API\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "The 3 classes with their associated methods that need to be overwritten are the following:\n",
    "* **MetaLearner**: The meta-learner contains the meta-algorithm logic. The `meta_fit(meta_train_generator, meta_valid_generator)` method has to be overwritten with your own meta-learning algorithm. It receives the data generators initialized with default setting or your **config.json** file. Note that `meta_valid_generator` is `None` if you define `validation_datasets` as `null`.\n",
    "* **Learner**: It encapsulates the logic to learn from a new unseen task. Several methods need to be overwritten: \n",
    " * `fit(dataset_train)`: Takes a `dataset_train` as an argument and fit the learner according to it. The `dataset_train` is a tuple that contains the support set images, support set labels, number of ways and number of shots for the current task.\n",
    " * `save(path)`: You need to implement a way to save your model in the specified directory. \n",
    " * `load(path)`: You need to implement a way to load your model from the file(s) you created in `save(path)`.\n",
    "* **Predictor**: The predictor contains the logic of your model to make predictions once the learner is fitted. The `predict(dataset_test)` encapsulates this step. In this case, the `dataset_test` is a `torch.Tensor` which corresponds to the unlabelled query set for the current task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough a submission example\n",
    "\n",
    "In this sub-section, we present how your code submission folder should look like before zipping it.  \n",
    "\n",
    "**Example of a submission directory**\n",
    "```\n",
    "random\n",
    "│   model.py    (Mandatory)\n",
    "|   metadata    (Mandatory)\n",
    "|   config.json (Optional but must have this name)\n",
    "│   helper.py   (Optional) \n",
    "│   utils.py    (Optional)\n",
    "│   ...\n",
    "```\n",
    "<code>model.py</code> and <code>metadata</code> are the crucial files to be added. The former is your learning algorithm following our challenge API and the <code>metadata</code> is just a file for the competition server to work properly, you simply add it to your folder without worrying about it (you can find this file in any given baseline's folder). Other files could be added which means that you are free to organize your code as you would like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the classes\n",
    "We will go through a dummy example to understand how to create a model. In the code cell below, you can find the **random** baseline. There are 2 important remarks:\n",
    "- First, it is mandatory to **write a file(s)** in the `path` given as an argument in the `save(path)` method. It could be a any file, some metadata that you gathered and/or your serialized neural network, but you need to include one.\n",
    "- Then, one can notice that the shape of the array returned by the `predict` method depends on the query set of each task.\n",
    "  \n",
    "**Note**: You can always test your algorithm with `run.py` to verify everything is working properly. We explain how to run the script in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This is a dummy baseline. It is just supposed to check if ingestion and \n",
    "scoring are called properly.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch import Tensor\n",
    "from typing import Iterable, Any, Tuple\n",
    "\n",
    "from cdmetadl.ingestion.data_generator import Task\n",
    "from cdmetadl.api.api import MetaLearner, Learner, Predictor\n",
    "\n",
    "SEED = 98\n",
    "\n",
    "class MyMetaLearner(MetaLearner):\n",
    "\n",
    "    def __init__(self, train_classes: int) -> None:\n",
    "        \"\"\" Defines the meta-learning algorithm's parameters. For example, one \n",
    "        has to define what would be the learner meta-learner's architecture. \n",
    "        \n",
    "        Args:\n",
    "            train_classes (int): Total number of classes that can be seen \n",
    "                during meta-training. If the data format during training is \n",
    "                'task', then this parameter corresponds to the number of ways, \n",
    "                while if the data format is 'batch', this parameter corresponds \n",
    "                to the total number of classes across all training datasets.\n",
    "        \"\"\"\n",
    "        super().__init__(train_classes)\n",
    "        self.seed = SEED\n",
    "\n",
    "    def meta_fit(self, \n",
    "                 meta_train_generator: Iterable[Any], \n",
    "                 meta_valid_generator: Iterable[Task]) -> Learner:\n",
    "        \"\"\" Uses the generators to tune the meta-learner's parameters. The \n",
    "        meta-training generator generates either few-shot learning tasks or \n",
    "        batches of images, while the meta-valid generator always generates \n",
    "        few-shot learning tasks.\n",
    "        \n",
    "        Args:\n",
    "            meta_train_generator: Function that generates the training data.\n",
    "                The generated can be a N-way k-shot task or a batch of images \n",
    "                with labels.\n",
    "            meta_valid_generator: Function that generates the validation data.\n",
    "                The generated data always come in form of N-way k-shot tasks.\n",
    "                \n",
    "        Returns:\n",
    "            Learner: Resulting learner ready to be trained and evaluated on new\n",
    "                unseen tasks.\n",
    "        \"\"\"\n",
    "        return MyLearner(self.seed)\n",
    "\n",
    "\n",
    "class MyLearner(Learner):\n",
    "\n",
    "    def __init__(self, seed: int = 0) -> None:\n",
    "        \"\"\" Defines the learner initialization.\n",
    "\n",
    "        Args:\n",
    "            seed (int, optional): Random seed. Defaults to 0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "\n",
    "    def fit(self, dataset_train: Tuple[Tensor, Tensor, int, int]) -> Predictor:\n",
    "        \"\"\" Fit the Learner to the support set of a new unseen task. \n",
    "        \n",
    "        Args:\n",
    "            dataset_train (Tuple[Tensor, Tensor, int, int]): Support set of a \n",
    "                task. The data arrive in the following format (X_train, \n",
    "                y_train, n_ways, k_shots). X_train is the tensor of labeled \n",
    "                imaged of shape [n_ways*k_shots x 3 x 128 x 128], y_train is \n",
    "                the tensor of encoded labels (Long) for each image in X_train \n",
    "                with shape of [n_ways*k_shots], n_ways is the number of classes \n",
    "                and k_shots the number of examples per class.\n",
    "                        \n",
    "        Returns:\n",
    "            Predictor: The resulting predictor ready to predict unlabelled \n",
    "                query image examples from new unseen tasks.\n",
    "        \"\"\"\n",
    "        _, y_train, _, _ = dataset_train\n",
    "        return MyPredictor(y_train, self.seed)\n",
    "\n",
    "    def save(self, path_to_save: str) -> None:\n",
    "        \"\"\" Saves the learning object associated to the Learner. \n",
    "        \n",
    "        Args:\n",
    "            path_to_save (str): Path where the learning object will be saved.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not os.path.isdir(path_to_save):\n",
    "            raise ValueError((\"The model directory provided is invalid. Please\"\n",
    "                + \" check that its path is valid.\"))\n",
    "        \n",
    "        pickle.dump(self, open(f\"{path_to_save}/learner.pickle\", \"wb\"))\n",
    " \n",
    "    def load(self, path_to_load: str) -> None:\n",
    "        \"\"\" Loads the learning object associated to the Learner. It should \n",
    "        match the way you saved this object in self.save().\n",
    "        \n",
    "        Args:\n",
    "            path_to_load (str): Path where the Learner is saved.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(path_to_load):\n",
    "            raise ValueError((\"The model directory provided is invalid. Please\"\n",
    "                + \" check that its path is valid.\"))\n",
    "        \n",
    "        model_file = f\"{path_to_load}/learner.pickle\"\n",
    "        if os.path.isfile(model_file):\n",
    "            with open(model_file, \"rb\") as f:\n",
    "                saved_learner = pickle.load(f)\n",
    "            self.seed = saved_learner.seed\n",
    "        \n",
    "    \n",
    "class MyPredictor(Predictor):\n",
    "\n",
    "    def __init__(self, \n",
    "                 labels: Tensor, \n",
    "                 seed: int) -> None:\n",
    "        \"\"\" Defines the Predictor initialization.\n",
    "\n",
    "        Args:\n",
    "            labels (Tensor): Tensor of encoded labels.\n",
    "            seed (int): Random seed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.labels = np.unique(labels.numpy())\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def predict(self, dataset_test: Tensor) -> np.ndarray:\n",
    "        \"\"\" Given a dataset_test, predicts the probabilities associated to the \n",
    "        provided images.\n",
    "        \n",
    "        Args:\n",
    "            dataset_test (Tensor): Tensor of unlabelled image examples of shape \n",
    "                [n_ways*query_size x 3 x 128 x 128].\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Predicted probs for all images. The array must be of \n",
    "                shape [n_ways*query_size, n_ways].\n",
    "        \"\"\"\n",
    "        random_pred = np.random.choice(self.labels, len(dataset_test))\n",
    "        # Mimic prediction probabilities\n",
    "        random_probs = np.zeros((random_pred.size, len(self.labels)))\n",
    "        random_probs[np.arange(random_pred.size), random_pred] = 1\n",
    "        return random_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refer to the <code>cd-metadl/baselines/</code> folder if you want to see submission examples. Here are the algorithms provided: \n",
    "- The **Random** baseline.\n",
    "- The **Train from scratch** baseline which learns every task starting from a random initialization at meta-test time, *i.e.*, no meta-learning.  \n",
    "- The **FineTuning** baseline which pre-trains a network with batches of data from the meta-training split and during meta-testing only fine-tunes the last layer.\n",
    "- The **Prototypical Networks** based on  [J. Snell et al. - Prototypical Networks for Few-shot Learning (2017)](https://arxiv.org/pdf/1703.05175).\n",
    "- The **Matching Networks** based on  [O. Vinyals et al. - Matching Networks for One Shot Learning (2017)](https://arxiv.org/abs/1606.04080).\n",
    "- The **MAML** algorithm based on [C. Finn et al. - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks (2017)](https://arxiv.org/pdf/1703.03400)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# III - Test and Submission\n",
    "\n",
    "Here we present the `run.py` script. It is meant to mimick what is happenning on the CodaLab platform, i.e. the competition server. Let's say you worked on an algorithm and you are ready to test it before submitting it. More specifically, it will create your MetaLearner object, run the meta-fit method and evaluate your meta-algorithm on test episodes generated from the meta-test split. You can run the script command with the following arguments:\n",
    "- `input_data_dir`: The path which contains the **public datasets**. \n",
    "- `submission_dir`: The path which contains your **algorithm's code** following the format we previously defined. \n",
    "- `overwrite_previous_results`: Boolean flag to control if the output folders should be overwritten or not.\n",
    "- `test_tasks_per_dataset`: Number of tasks per dataset during the meta-testing stage. In the competition, during the feedback phase this parameter is set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m cdmetadl.run \\\n",
    "    --input_data_dir=../public_data \\\n",
    "    --submission_dir=../baselines/random \\\n",
    "    --overwrite_previous_results=True \\\n",
    "    --test_tasks_per_dataset=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a ZIP file ready for submission\n",
    "Here we present how to zip your code to submit it on the CodaLab platform. As an example, we zip the folder <code>cd-metadl/baselines/random/</code> which corresponds to the random baseline which was introduced in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zip_utils import zipdir\n",
    "\n",
    "model_dir = \"../baselines/random/\"\n",
    "submission_filename = \"mysubmission.zip\"\n",
    "zipdir(submission_filename, model_dir)\n",
    "print(f\"Submit this file: {submission_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "For clarity, we summarize the steps that you should be aware of while making a submission : \n",
    "- Follow the **MetaLearner**/**Learner**/**Predictor** API to encapsulate your few-shot learning algorithm. Please make sure you name your subclasses as **MyMetaLearner**, **MyLearner** and **MyPredictor** respectively.\n",
    "- Make sure you <u>save</u> at least a file in the given <code>path</code>. If this is a trained neural network, you need to serialize it in the <code>save()</code> method, and provide code to deserialize it in the <code>load()</code> method. Examples are provided in <code>cd-metadl/baselines/</code>.\n",
    "- In your algorithm folder, make sure you have <code>model.py</code> and <code>metadata</code> with these **exact** names. If you want to use your custom configuration for the training generator make sure to include the **config.json** file.\n",
    "\n",
    "--- \n",
    "\n",
    "## Next steps\n",
    "Now you know all the steps required to create a valid code submission.\n",
    "\n",
    "Good luck !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2b67b475aa37c9332a01aec187d4c41db87ea08566513aed1ffebce2cf2c6057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
